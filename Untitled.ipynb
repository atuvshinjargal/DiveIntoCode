{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'maths'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-57a7b42e3fe3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mturtle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmaths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'maths'"
     ]
    }
   ],
   "source": [
    "from turtle import forward\n",
    "import numpy as np\n",
    "import maths\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    " \n",
    "class FC():\n",
    "    \"\"\"\n",
    "    FC layers from number of nodes n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    --------------\n",
    "    n_nodes1 : int\n",
    "        Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "        Number of nodes in later layer\n",
    "    initializer : Instances of initialization methods\n",
    "    optimizer : Instances of optimization methods\n",
    "    activation : Activation function\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, activation):\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward\n",
    "        Parameters\n",
    "        ----------------\n",
    "        X : ndarray shape with (batch_size, n_nodes1)\n",
    "            Input\n",
    "        Returns\n",
    "        ----------------\n",
    "        A : ndarray shape with (batch_size, n_nodes2)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        A = X @ self.W + self.B\n",
    "        return self.activation.forward(A)\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------------\n",
    "        dA : ndarray shape with (batch_size, n_nodes2)\n",
    "            The gradient flowed in from behind\n",
    "        Returns\n",
    "        ----------------\n",
    "        dZ : ndarray shape with (batch_size, n_nodes1)\n",
    "            forward slope\n",
    "        \"\"\"\n",
    "        dA = self.activation.backward(dA)\n",
    "        dZ = dA@self.W.T\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = self.X.T@dA\n",
    "        self.optimizer.update(self)\n",
    "        return dZ\n",
    "\n",
    "\n",
    "class SimpleConv2d():\n",
    "    def __init__(self, F, C, FH, FW, P, S,initializer=None,optimizer=None,activation=None):\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.W = self.initializer.W(F,C,FH,FW)\n",
    "        self.B = self.initializer.B(F)\n",
    "    def output_shape2d(self,H,W,PH,PW,FH,FW,SH,SW):\n",
    "        OH = (H +2*PH -FH)/SH +1\n",
    "        OW = (W +2*PW -FW)/SW +1\n",
    "        return int(OH),int(OW)\n",
    "    \n",
    "    def forward(self, X,debug=False):\n",
    "        self.X = X\n",
    "        N,C,H,W = self.X.shape\n",
    "        \n",
    "        F,C,FH,FW = self.W.shape\n",
    "        OH,OW = self.output_shape2d(H,W,self.P,self.P,FH,FW,self.S,self.S)\n",
    "        self.params = N,C,H,W,F,FH,FW,OH,OW\n",
    "        A = np.zeros([N,F,OH,OW])\n",
    "        self.X_pad = np.pad(self.X,((0,0),(0,0),(self.P,self.P),(self.P,self.P)))\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(0,H,self.S):\n",
    "                    for col in range(0,W,self.S):\n",
    "                        if self.P == 0 and (W-2 <= col or H-2<=row):\n",
    "                            continue\n",
    "                        A[n,ch,row,col] = np.sum(self.X_pad[n,:,row:row+FH,col:col+FW]*self.W[ch,:,:,:]) +self.B[ch]\n",
    "        if debug==True:\n",
    "            return A\n",
    "        else:\n",
    "            return  self.activation.forward(A)\n",
    "    \n",
    "    def backward(self, dZ,debug=False):\n",
    "        if debug==True:\n",
    "            dA = dZ\n",
    "        else:\n",
    "            dA = self.activation.backward(dZ)\n",
    "        N,C,H,W,F,FH,FW,OH,OW = self.params\n",
    "        dZ = np.zeros(self.X_pad.shape)\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.dB = np.zeros(self.B.shape)\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(0,H,self.S):\n",
    "                    for col in range(0,W,self.S):\n",
    "                        if self.P == 0 and (W-2 <= col or H-2<=row):\n",
    "                            continue\n",
    "                        dZ[n,:,row:row+FH,col:col+FW] += dA[n,ch,row,col]*self.W[ch,:,:,:]\n",
    "        if self.P == 0:\n",
    "            dZ = np.delete(dZ,[0,H-1],axis=2)\n",
    "            dZ = np.delete(dZ,[0,W-1],axis=3)\n",
    "        else:\n",
    "            dl_rows = range(self.P),range(H+self.P,H+2*self.P,1)\n",
    "            dl_cols = range(self.P),range(W+self.P,W+2*self.P,1)\n",
    "            dZ = np.delete(dZ,dl_rows,axis=2)\n",
    "            dZ = np.delete(dZ,dl_cols,axis=3)\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(OH):\n",
    "                    for col in range(OW):\n",
    "                        self.dW[ch,:,:,:] += dA[n,ch,row,col]*self.X_pad[n,:,row:row+FH,col:col+FW]\n",
    "        for ch in range(F):\n",
    "            self.dB[ch] = np.sum(dA[:,ch,:,:])\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "\n",
    "class SimpleInitializerConv2d:\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "    def W(self, F, C, FH, FW):\n",
    "        return self.sigma * np.random.randn(F,C,FH,FW)\n",
    "    def B(self, F):\n",
    "        return np.zeros(F)\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.clip(A, 0, None)\n",
    "    def backward(self, dZ):\n",
    "        return dZ * np.clip(np.sign(self.A), 0, None)\n",
    "    \n",
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "class MaxPool2D():\n",
    "    def __init__(self,P):\n",
    "        self.P = P\n",
    "        self.PA = None\n",
    "        self.Pindex = None\n",
    "    def forward(self,A):\n",
    "        N,F,OH,OW = A.shape\n",
    "        PH,PW = int(OH/self.P),int(OW/self.P)\n",
    "        self.params = N,F,OH,OW,self.P,PH,PW\n",
    "        self.PA = np.zeros([N,F,PH,PW])\n",
    "        self.Pindex = np.zeros([N,F,PH,PW])\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        self.PA[n,ch,row,col] =np.max(A[n,ch,row*self.P:row*self.P+self.P,col*self.P:col*self.P+self.P])\n",
    "                        self.Pindex[n,ch,row,col] = np.argmax(A[n,ch,row*self.P:row*self.P+self.P,col*self.P:col*self.P+self.P])\n",
    "        return self.PA\n",
    "    def backward(self,dA):\n",
    "        N,F,OH,OW,PS,PH,PW = self.params\n",
    "        dP = np.zeros([N,F,OH,OW])\n",
    "        for n in range(N): \n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        idx = self.Pindex[n,ch,row,col]\n",
    "                        tmp = np.zeros((PS*PS))\n",
    "                        for i in range(PS*PS):\n",
    "                            if i == idx:\n",
    "                                tmp[i] = dA[n,ch,row,col]\n",
    "                            else:\n",
    "                                tmp[i] = 0\n",
    "                        dP[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS] = tmp.reshape(PS,PS)\n",
    "        return dP\n",
    "\n",
    "class Scratch2dCNNClassifier():\n",
    "    def __init__(self, NN, CNN, n_epoch=5, n_batch=1, verbose = False):\n",
    "        self.NN = NN\n",
    "        self.CNN = CNN\n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_batch = n_batch\n",
    "        self.verbose = verbose\n",
    "        self.log_loss = np.zeros(self.n_epoch)\n",
    "        self.log_acc = np.zeros(self.n_epoch)\n",
    "\n",
    "    def loss_function(self,y,yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt*np.log(y+delta))\n",
    "    def accuracy(self,Z,Y):\n",
    "        return accuracy_score(Y,Z)\n",
    "\n",
    "    def fit(self, X, y, X_val=False, y_val=False):\n",
    "        for epoch in range(self.n_epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
    "            self.loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:              \n",
    "                forward_data = mini_X_train[:,np.newaxis,:,:]\n",
    "                for layer in range(len(self.CNN)):\n",
    "                    forward_data = self.CNN[layer].forward(forward_data)\n",
    "                flt = Flatten()\n",
    "                forward_data = flt.forward(forward_data)\n",
    "                for layer in range(len(self.NN)):\n",
    "                    forward_data = self.NN[layer].forward(forward_data)\n",
    "                Z = forward_data\n",
    "                backward_data = (Z - mini_y_train)/self.n_batch\n",
    "                for layer in range(len(self.NN)-1,-1,-1):\n",
    "                    backward_data = self.NN[layer].backward(backward_data)\n",
    "                backward_data = flt.backward(backward_data)\n",
    "                for layer in range(len(self.CNN)-1,-1,-1):\n",
    "                    backward_data = self.CNN[layer].backward(backward_data)\n",
    "                self.loss += self.loss_function(Z,mini_y_train)\n",
    "                if self.verbose:\n",
    "                    print('batch loss %f'%self.loss_function(Z,mini_y_train))\n",
    "            if self.verbose:\n",
    "                print(self.loss/len(get_mini_batch),self.accuracy(self.predict(X),np.argmax(y,axis=1)))\n",
    "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
    "            self.log_acc[epoch] = self.accuracy(self.predict(X),np.argmax(y,axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_data = X[:,np.newaxis,:,:]\n",
    "        for layer in range(len(self.CNN)):\n",
    "            pred_data = self.CNN[layer].forward(pred_data)\n",
    "        flt=Flatten()\n",
    "        pred_data = flt.forward(pred_data)\n",
    "        for layer in range(len(self.NN)):\n",
    "            pred_data = self.NN[layer].forward(pred_data)\n",
    "        return np.argmax(pred_data,axis=1)\n",
    "\n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
    "        return self.Z\n",
    "    def backward(self, Y):\n",
    "        self.loss = self.loss_func(Y)\n",
    "        return self.Z - Y\n",
    "    def loss_func(self, Y, Z=None):\n",
    "        if Z is None:\n",
    "            Z = self.Z\n",
    "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.HW = 1\n",
    "        self.HB = 1\n",
    "    def update(self, layer):\n",
    "        self.HW += layer.dW**2\n",
    "        self.HB += layer.dB**2\n",
    "        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n",
    "        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, *shape):\n",
    "        W = self.sigma * np.random.randn(*shape)\n",
    "        return W\n",
    "    def B(self, *shape):\n",
    "        B = self.sigma * np.random.randn(*shape)\n",
    "        return B\n",
    "class HeInitializer():\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = math.sqrt(2 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B\n",
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1] \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "class Tanh:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    def backward(self, dZ):\n",
    "        return dZ * (1 - (np.tanh(self.A))**2)\n",
    "\n",
    "\n",
    "############### Problem 5 ################\n",
    "# (Advance task) Creating an average pooling\n",
    "class AvgPool2D():\n",
    "    '''\n",
    "    Perform average pooling\n",
    "    Parameters\n",
    "    --------------------\n",
    "    P : int \n",
    "         average pooling size\n",
    "    '''\n",
    "    def __init__(self, P):\n",
    "        self.P = P\n",
    "        self.PA = None\n",
    "        self.Pindex = None\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        forward\n",
    "        Parameters\n",
    "        -------------------\n",
    "        A : ndarray shape with(n_batch, filter, height and width)\n",
    "            training samples\n",
    "        \n",
    "        \"\"\"\n",
    "        N,F,OH,OW = A.shape\n",
    "        PS = self.P\n",
    "        PH,PW = int(OH/PS), int(OW/PS)\n",
    "\n",
    "        self.params = N,F,OH,OW,PS,PH,PW\n",
    "\n",
    "        # pooling filter\n",
    "        self.PA = np.zeros([N,F,PH,PW])\n",
    "\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        self.PA[n,ch,row,col] = np.mean(A[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS])\n",
    "\n",
    "        return self.PA\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        N,F,OH,OW,PS,PH,PW = self.params\n",
    "        dP = np.zeros([N,F,OH,OW])\n",
    "\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        tmp = np.zeros((PS*PS))\n",
    "                        for i in range(PS*PS):\n",
    "                                tmp[i] = dA[n,ch,row,col]/(PS*PS)\n",
    "                        dP[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS] = tmp.reshape(PS,PS)\n",
    "        return dP\n",
    "############### Problem 6 ##################\n",
    "# Smoothing\n",
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(len(X),-1)\n",
    "    def backward(self, X):\n",
    "        return X.reshape(self.shape)\n",
    "\n",
    "################# Data set preparation ###############\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"X_train data shape: \", X_train.shape) # (60000, 28, 28)\n",
    "print(\"X_test data shape: \", X_test.shape) # (10000, 28, 28)\n",
    "\n",
    "# Preprocessing\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# the correct label is an integer from 0 to 9, but it is converted to a one-hot representation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train.reshape(-1,1))\n",
    "y_val_one_hot = enc.fit_transform(y_val.reshape(-1,1))\n",
    "\n",
    "print(\"Train dataset:\", X_train.shape) # (48000, 784)\n",
    "print(\"Validation dataset:\", X_val.shape) # (12000, 784)\n",
    "\n",
    "############### Problem 2 & 3 ################\n",
    "# Experiment of a two-dimensional convolution layer with a small array\n",
    "def output_shape2d(H,W,PH,PW,FH,FW,SH,SW):\n",
    "    OH = (H +2*PH -FH)/SH +1\n",
    "    OW = (W +2*PW -FW)/SW +1\n",
    "    return int(OH),int(OW)\n",
    "\n",
    "x = np.array([[[[ 1,  2,  3,  4],\n",
    "                [ 5,  6,  7,  8],\n",
    "                [ 9, 10, 11, 12],\n",
    "                [13, 14, 15, 16]]]])\n",
    "\n",
    "w = np.array([[[[ 0.,  0.,  0.],\n",
    "               [ 0.,  1.,  0.],\n",
    "               [ 0., -1.,  0.]]],\n",
    "\n",
    "              [[[ 0.,  0.,  0.],\n",
    "               [ 0., -1.,  1.],\n",
    "               [ 0.,  0.,  0.]]]])\n",
    "\n",
    "#w = np.array([[[[ 0.,  0.,  0.],\n",
    "#               [ 0.,  1.,  0.],\n",
    "#               [ 0., -1.,  0.]]]])\n",
    "\n",
    "#w = w[:,np.newaxis,:,:]\n",
    "N,C,H,W = x.shape\n",
    "F,C,FH,FW = w.shape\n",
    "S = 1\n",
    "P = 1\n",
    "#w = np.ones([F,C,FH,FW])\n",
    "b = np.ones((C,F))\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"w shape\", w.shape)\n",
    "#print(w)\n",
    "\n",
    "OH,OW = output_shape2d(H,W,P,P,FH,FW,S,S)\n",
    "X_pad = np.pad(x,((0,0),(0,0),(P,P),(P,P)))\n",
    "print(\"x pad:\", X_pad)\n",
    "\"\"\"\n",
    "#### forward ####\n",
    "A = np.zeros([N,C,OH,OW])\n",
    "for n in range(N):\n",
    "    for ch in range(C):\n",
    "        for row in range(0,H,S):\n",
    "            for col in range(0,W,S):\n",
    "                A[n,ch,row,col] = np.sum(X_pad[n,:,row:row+FH, col:col+FW] * w[:,ch,:,:]) + b[ch]\n",
    "print(\"A shape:\",A.shape)\n",
    "print(\"A:\", A)\n",
    "print(\"X_pad shape:\", X_pad.shape)\n",
    "#### Backward\n",
    "\n",
    "\n",
    "\n",
    "# (1,1,4,4)\n",
    "x = np.array([[[[ 1,  2,  3,  4],\n",
    "                [ 5,  6,  7,  8],\n",
    "                [ 9, 10, 11, 12],\n",
    "                [13, 14, 15, 16]]]])\n",
    "\n",
    "# (2,3,3)\n",
    "w = np.array([[[ 0.,  0.,  0.],\n",
    "               [ 0.,  1.,  0.],\n",
    "               [ 0., -1.,  0.]],\n",
    "\n",
    "              [[ 0.,  0.,  0.],\n",
    "               [ 0., -1.,  1.],\n",
    "               [ 0.,  0.,  0.]]])\n",
    "               \"\"\"\n",
    "simple_conv_2d = SimpleConv2d(F=2, C=1, FH=3, FW=3, P=0, S=1,initializer=SimpleInitializerConv2d(),optimizer=SGD(0.01),activation=ReLU())\n",
    "simple_conv_2d.W = w\n",
    "print(x.shape)\n",
    "print(w.shape)\n",
    "A = simple_conv_2d.forward(x,True)\n",
    "print(A)\n",
    "\n",
    "#da = np.array([[[[ -4,  -4], [ 10,  11]],[[  1,  -7],[  1, -11]]]])\n",
    "delta = np.array([[[[ -4,  -4],\n",
    "                   [ 10,  11]],\n",
    "\n",
    "                  [[  1,  -7],\n",
    "                   [  1, -11]]]])\n",
    "dZ = simple_conv_2d.backward(delta,True)\n",
    "print(dZ)\n",
    "\n",
    "\n",
    "result =simple_conv_2d.output_shape2d(H=6,W=6,PH=0,PW=0,FH=3,FW=3,SH=1,SW=1)\n",
    "print(result)\n",
    "\n",
    "################ Problem 4 test ##################\n",
    "test_data = np.random.randint(0,9,36).reshape(1,1,6,6)\n",
    "maxpooling = MaxPool2D(P=2)\n",
    "pool_forward = maxpooling.forward(test_data)\n",
    "print(\"test data:\", test_data)\n",
    "print(\"Maxpooling forward:\", pool_forward)\n",
    "################ Problem 5 test ##################\n",
    "test_data = np.random.randint(0,9,36).reshape(1,1,6,6)\n",
    "avgpooling = AvgPool2D(P=2)\n",
    "pool_forward = avgpooling.forward(test_data)\n",
    "print(\"test data:\", test_data)\n",
    "print(\"Avgpooling forward:\", pool_forward)\n",
    "################ Problem 6 test ##################\n",
    "test_data = np.zeros([10,2,5,5])\n",
    "flat = Flatten()\n",
    "flat_forward = flat.forward(test_data)\n",
    "flat_backward = flat.backward(flat_forward)\n",
    "print(\"test data shape:\", test_data.shape)\n",
    "print(\"Flat forward shape:\", flat_forward.shape)\n",
    "print(\"Flat backward shape:\", flat_backward.shape)\n",
    "############### Problem 7 ###################\n",
    "# Learning and estimation\n",
    "\"\"\"\n",
    "NN = {\n",
    "    0:FC(1960, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "    1:FC(200, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "    2:FC(200, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),\n",
    "}\n",
    "CNN = {\n",
    "    0:SimpleConv2d(F=10, C=1, FH=3, FW=3, P=1, S=1,initializer=SimpleInitializerConv2d(),optimizer=SGD(0.01),activation=ReLU()),\n",
    "    1:MaxPool2D(2),\n",
    "}\n",
    "\"\"\" \n",
    "NN = {0:FC(7840, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "    1:FC(200, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "    2:FC(200, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),}\n",
    "\n",
    "CNN = {0: SimpleConv2d(F=10, C=1,FH=3,FW=3,P=1,S=1,\n",
    "            initializer=SimpleInitializerConv2d(0.01), optimizer=SGD(0.1), activation=ReLU()),}\n",
    "\n",
    "cnn2d = Scratch2dCNNClassifier(NN=NN, CNN=CNN, n_epoch=1, n_batch=20, verbose = True)\n",
    "cnn2d.fit(X_train[0:1000], y_train_one_hot[0:1000])\n",
    "\n",
    "y_pred = cnn2d.predict(X_val[0:500])\n",
    "acc = accuracy_score(y_val[0:500], y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "############# Problem 8 #################\n",
    "#  (advanced task) LeNet\n",
    "\n",
    "LeNetCNN = {0: SimpleConv2d(F=6, C=1,FH=5,FW=5,P=2,S=1,\n",
    "            initializer=SimpleInitializerConv2d(0.01), optimizer=SGD(0.1), activation=ReLU()),\n",
    "            1: MaxPool2D(P=2),\n",
    "            2: SimpleConv2d(F=16, C=6,FH=5,FW=5,P=2,S=1,\n",
    "            initializer=SimpleInitializerConv2d(0.01), optimizer=SGD(0.1), activation=ReLU()),\n",
    "            3: MaxPool2D(P=2),}\n",
    "\n",
    "LeNetNN = {0: FC(784, 120, HeInitializer(), AdaGrad(0.01), Tanh()),\n",
    "    1: FC(120, 84, HeInitializer(), AdaGrad(0.01), Tanh()),\n",
    "    2: FC(84, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),}\n",
    "\n",
    "LeNet = Scratch2dCNNClassifier(NN = LeNetNN, CNN = LeNetCNN, n_epoch=10, n_batch=100, verbose=True)\n",
    "LeNet.fit(X_train[0:1000], y_train_one_hot[0:1000])\n",
    "\n",
    "y_pred_lenet = LeNet.predict(X_val[0:500])\n",
    "acc_lenet = accuracy_score(y_val[0:500], y_pred_lenet)\n",
    "print(\"Accuracy:\", acc_lenet)\n",
    "############## Problem 10 ##############\n",
    "# Calculation of output size and number of parameters\n",
    "print(\"Parameters in general are weights that are learnt during training. Parameters can calculate using following formula:\\n\\\n",
    "    (filter width*filter height*number of filter in the previous layer +1)* number of filters\")\n",
    "print(\"Example 1:\")\n",
    "print(\"input size: 144x144, 3\")\n",
    "print(\"Filter size: 3x3, 6\")\n",
    "print(\"Stride: 1\")\n",
    "print(\"Padding: None\")\n",
    "print(\"Number of parameter: 168\")\n",
    "print(\"output size: 142x142x6\")\n",
    "\n",
    "print(\"Example 2:\")\n",
    "print(\"input size: 60x60, 24\")\n",
    "print(\"Filter size: 3x3, 48\")\n",
    "print(\"Stride: 1\")\n",
    "print(\"Padding: None\")\n",
    "print(\"Number of parameter: 10416\")\n",
    "print(\"output size: 58x58x48\")\n",
    "\n",
    "print(\"Example 3:\")\n",
    "print(\"input size: 20x20, 10\")\n",
    "print(\"Filter size: 3x3, 20\")\n",
    "print(\"Stride: 2\")\n",
    "print(\"Padding: None\")\n",
    "print(\"Number of parameter: 1820\")\n",
    "print(\"output size: 9x9x20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
