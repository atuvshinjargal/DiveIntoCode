{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 14\n",
      "Max sequence length for outputs: 59\n",
      "WARNING:tensorflow:From c:\\Users\\NewTech\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\NewTech\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\NewTech\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\NewTech\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\NewTech\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\NewTech\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\Users\\NewTech\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 1.1856 - acc: 0.7344 - val_loss: 1.0136 - val_acc: 0.7174\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.8530 - acc: 0.7697 - val_loss: 0.8225 - val_acc: 0.7731\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.6774 - acc: 0.8110 - val_loss: 0.6936 - val_acc: 0.8036\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.5811 - acc: 0.8314 - val_loss: 0.6467 - val_acc: 0.8106\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.5295 - acc: 0.8451 - val_loss: 0.5737 - val_acc: 0.8328\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Atentez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Atentez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Atentez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Atentez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Estenez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Estenez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui se chente ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Laissez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Laissez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Laissez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: Atentez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: Atentez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: Atentez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Atentez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Atenez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Hide.\n",
      "Decoded sentence: Atenez-le courer.\n",
      "\n",
      "-\n",
      "Input sentence: Hide.\n",
      "Decoded sentence: Atenez-le courer.\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Atrenez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Atrendez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Artenez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Artenez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Artenez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attenez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attenez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attenez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Begin.\n",
      "Decoded sentence: Artenez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Begin.\n",
      "Decoded sentence: Artenez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Attez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Attez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Hello.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Hello.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Hello.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Hello.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je suis partente.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je suis partente.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Il est parte.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je suis partente.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je suis partente.\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: Je suis partente.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Estes-le chente ?\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Artez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Sorry?\n",
      "Decoded sentence: Atentez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Buy it.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Buy it.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Buy it.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Buy it.\n",
      "Decoded sentence: Atrendez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Atentez le cherre.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Atentez le cherre.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Atentez le cherre.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Atentez le cherre.\n",
      "\n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: Arrendez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: Arrendez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Artez le charter.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Artez-le !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 5  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy  as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'coco'...\n",
      "'make' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "python: can't open file 'setup.py': [Errno 2] No such file or directory\n",
      "python: can't open file 'setup.py': [Errno 2] No such file or directory\n",
      "Cloning into 'pytorch-tutorial'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pdollar/coco.git\n",
    "!cd coco/PythonAPI/\n",
    "!make\n",
    "!python setup.py build\n",
    "!python setup.py install\n",
    "!cd ../../\n",
    "!git clone https://github.com/yunjey/pytorch-tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NewTech\\OneDrive\\Documents\\Python Scripts\\pytorch-tutorial\\tutorials\\03-advanced\\image_captioning\n"
     ]
    }
   ],
   "source": [
    "# !cd pytorch-tutorial/tutorials/03-advanced/image_captioning/\n",
    "os.chdir('./pytorch-tutorial/tutorials/03-advanced/image_captioning/')\n",
    "# カレントディレクトリの取得\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\NewTech\\\\OneDrive\\\\Documents\\\\Python Scripts', 'c:\\\\Users\\\\NewTech\\\\anaconda3\\\\envs\\\\myenv\\\\python37.zip', 'c:\\\\Users\\\\NewTech\\\\anaconda3\\\\envs\\\\myenv\\\\DLLs', 'c:\\\\Users\\\\NewTech\\\\anaconda3\\\\envs\\\\myenv\\\\lib', 'c:\\\\Users\\\\NewTech\\\\anaconda3\\\\envs\\\\myenv', '', 'c:\\\\Users\\\\NewTech\\\\anaconda3\\\\envs\\\\myenv\\\\lib\\\\site-packages', 'c:\\\\Users\\\\NewTech\\\\anaconda3\\\\envs\\\\myenv\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\NewTech\\\\anaconda3\\\\envs\\\\myenv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\NewTech\\\\anaconda3\\\\envs\\\\myenv\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\NewTech\\\\anaconda3\\\\envs\\\\myenv\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\NewTech\\\\.ipython', '/lib/python3.7/site-packages', './pytorch-tutorial/tutorials/03-advanced/image_captioning/', '/lib/python3.7/site-packages', '/lib/python3.7/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append('/lib/python3.7/site-packages')\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from -r requirements.txt (line 1)) (3.5.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from -r requirements.txt (line 2)) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from -r requirements.txt (line 3)) (1.21.6)\n",
      "Requirement already satisfied: Pillow in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from -r requirements.txt (line 4)) (9.5.0)\n",
      "Collecting argparse\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 1)) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk->-r requirements.txt (line 2)) (2023.5.5)\n",
      "Requirement already satisfied: click in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk->-r requirements.txt (line 2)) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->-r requirements.txt (line 1)) (4.6.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from click->nltk->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from click->nltk->-r requirements.txt (line 2)) (6.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\newtech\\anaconda3\\envs\\myenv\\lib\\site-packages (from importlib-metadata->click->nltk->-r requirements.txt (line 2)) (3.15.0)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
   {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10338,
     "status": "ok",
     "timestamp": 1585901334752,
     "user": {
      "displayName": "柴田祐希",
      "photoUrl": "",
      "userId": "14368319687983657863"
     },
     "user_tz": -540
    },
    "id": "fD0LYE0_f7bc",
    "outputId": "84abf3ad-7f21-4f21-f72a-ee8c3b4f9e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> a group of giraffes standing next to each other . <end>\n"
     ]
    }
   ],
   "source": [
    "# 5. Test the model\n",
    "!python sample.py --image='png/example.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9257,
     "status": "ok",
     "timestamp": 1585901639772,
     "user": {
      "displayName": "柴田祐希",
      "photoUrl": "",
      "userId": "14368319687983657863"
     },
     "user_tz": -540
    },
    "id": "ON7qmn1JhUU1",
    "outputId": "d3e21020-7057-4582-da8f-0789390b3999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> a man holding a hot dog in a bun . <end>\n"
     ]
    }
   ],
   "source": [
    "# ホットドッグを食べている人の画像\n",
    "!python sample.py --image='png/images.jpeg'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Problem 3] \n",
    "\n",
    "Investigate what to do if you want to run with Keras\n",
    "I ran the PyTorch implementation, but for some reason I want to run it in Keras. Investigate what steps will be taken and explain in as much detail as possible.\n",
    "\n",
    "Especially this time I want to be able to use pre-trained weights for PyTorch in Keras, so be sure to mention that.\n",
    "\n",
    "When using learned weights in keras\n",
    "\n",
    "Convert and save PyTorch learned weights to ONNX format (using torch.onnx.export)\n",
    "\n",
    "Convert ONNX format to a format that can be used by keras (using onnx2keras.onnx_to_keras)\n",
    "\n",
    "Convert channel-first to channel-last (PyTorch channel-first, onnx_to_kera change_ordering=True)\n",
    "\n",
    "(Depending on the layer, it may not be possible to convert from PyTorch to keras format, so adjust the layer so that it can be converted in advance)\n",
    "\n",
    "Scratch similar encoders, decoders, etc. in keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
