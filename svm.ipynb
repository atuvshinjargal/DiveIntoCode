{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradient_descent(self, X, y):\n",
    "     \"\"\"\n",
    "     X : ndarray of the following form, shape (n_samples, n_features)\n",
    "       learning data\n",
    "    \n",
    "     y : ndarray of the form, shape (n_samples)\n",
    "       training data results\n",
    "      \n",
    "     self.lam : float\n",
    "       regularization parameter\n",
    "      \n",
    "     self.lr : float\n",
    "       learning rate\n",
    "    \n",
    "     self.coef_ : ndarray of the following form, shape (n_features)\n",
    "       parameters for X\n",
    "      \n",
    "     coef_reg : ndarray of the following form, shape (n_features)\n",
    "       parameters used for regularization\n",
    "    \n",
    "     self.h0 : ndarray of the following form, shape (n_samples)\n",
    "       Predicted value of train by sigmoid function (process function)\n",
    "    \n",
    "     Returns\n",
    "     -------\n",
    "     self.theta_ : ndarray of the following form, shape (n_samples, 1)\n",
    "       Estimation result by linear assumption function\n",
    "      \n",
    "     \"\"\"\n",
    "     # configure kernel\n",
    "     if self.kernel=='linear':\n",
    "         K = np.dot(X, X.T)\n",
    "     elif self.kernel=='poly':\n",
    "         K = self.gamma*((np.dot(X, X.T) + self.theta)**self.d)\n",
    "        \n",
    "     # Tilt\n",
    "     delta = 1 - y* np.dot(K.T, self.coef_*y)\n",
    "\n",
    "# for version\n",
    "# delta = np.zeros(X.shape[0])\n",
    "# for i in range(X.shape[0]):\n",
    "# # Columns of K are multiplied by vectors of y and coef_: delta[i] = np.sum(self.coef_[i]*y[i]*K[:, i])\n",
    "# delta[i] = np.sum(self.coef_*y*K[:, i])\n",
    "# delta = 1 - y * delta\n",
    "        \n",
    "     # update formula\n",
    "     self.coef_ = self.coef_ + self.lr*delta\n",
    "    \n",
    "     # Since self.coef_ is greater than or equal to 0, set elements less than or equal to 0 to 0\n",
    "     self.coef_ = np.where(self.coef_<0, 0, self.coef_)\n",
    "    \n",
    "     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class ScratchSVMClassifier():\n",
    "    \"\"\"\n",
    "    Scratch implementation of SVM classifier\n",
    "\n",
    "     Parameters\n",
    "     ----------\n",
    "     num_iter : int\n",
    "       number of iterations\n",
    "     lr : float\n",
    "       learning rate\n",
    "     kernel : str\n",
    "       kernel type. Linear kernel (linear) or polynomial kernel (poly)\n",
    "     threshold : float\n",
    "       threshold for choosing support vectors\n",
    "     verbose : bool\n",
    "       True to print the learning process\n",
    "\n",
    "     Attributes\n",
    "     ----------\n",
    "     self.n_support_vectors : int\n",
    "       number of support vectors\n",
    "     self.index_support_vectors : ndarray of the following form, shape (n_support_vectors,)\n",
    "       support vector index\n",
    "     self.X_sv : ndarray of the form, shape(n_support_vectors, n_features)\n",
    "       Support vector features\n",
    "     self.lam_sv : ndarray of the form, shape(n_support_vectors, 1)\n",
    "       undetermined multiplier of the support vector\n",
    "     self.y_sv : ndarray of the form, shape(n_support_vectors, 1)\n",
    "       support vector labels\n",
    "\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter, lr, kernel='linear',  gamma=1,\n",
    "                 theta=0, d=1, threshold=1e-5, verbose=False):\n",
    "        # Record hyperparameters as attributes\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.kernel = kernel\n",
    "        self.threshold = threshold\n",
    "        self.verbose = verbose\n",
    "        # support vectors\n",
    "        self.s_X = 0\n",
    "        self.s_y = 0\n",
    "        # parameters of the polynomial kernel\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.d = d\n",
    "        # Initialize parameters and predictors\n",
    "        self. coef_ = 0\n",
    "        # Initialize category max and min of y\n",
    "        self. y_max = 0\n",
    "        self. y_min = 0\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        SVM分類器を学習する。検証データが入力された場合はそれに対する精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # make y one-dimensional\n",
    "        y = y.ravel()\n",
    "        y_val = y_val.ravel()\n",
    "    \n",
    "        # get the category maximum and minimum of y\n",
    "        self.y_max = y.max()\n",
    "        self.y_min = y.min()\n",
    "    \n",
    "        # -1, 1 of y\n",
    "        # \"-1, 1\" instead of \"0, 1\" for support vectors\n",
    "        if self.y_max == 2:\n",
    "            y = np.where(y==2, 1, -1)\n",
    "            y_val = np.where(y_val==2, 1, -1)\n",
    "        else:\n",
    "            y = np.where(y==1, 1, -1)\n",
    "            y_val = np.where(y_val==1, 1, -1)\n",
    "    \n",
    "        # Set parameters (normal distribution with mean 0 and variance 1)\n",
    "        # \"Normal distribution ✖️0.001\" because it is easy to converge when it is small\n",
    "        self.coef_ = np.random.normal(0, 1, X.shape[0])*0.01\n",
    "    \n",
    "        for iter_count in range(self.iter):\n",
    "            if self.verbose:\n",
    "                #Output learning process when #verbose is set to True\n",
    "                print(\"{}th learning\".format(iter_count))\n",
    "        \n",
    "            # steepest descent\n",
    "            # update self.coef_\n",
    "            _gradient_descent(self, X, y)\n",
    "    \n",
    "        # Remove 0 elements (so that they can be used when estimating)\n",
    "        coef_index = np.where(self.coef_>self.threshold)\n",
    "        # Determination of support vectors\n",
    "        # Since the above index is a tuple, it is not necessary to specify the second dimension of X)\n",
    "        self.coef_ = self.coef_[coef_index]\n",
    "        self.s_X = X[coef_index]\n",
    "        self.s_y = y[coef_index]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "         Estimate labels using SVM classifier.\n",
    "\n",
    "         Parameters\n",
    "         ----------\n",
    "         X : ndarray of the following form, shape (n_samples, n_features)\n",
    "             sample\n",
    "\n",
    "         Returns\n",
    "         -------\n",
    "             An ndarray of the form, shape (n_samples, 1)\n",
    "             Estimation result by SVM classifier\n",
    "         \"\"\"\n",
    "         # configure kernel\n",
    "        if self.kernel=='linear':\n",
    "            K = np.dot(X, self.s_X.T)\n",
    "        elif self.kernel=='poly':\n",
    "            K = self.gamma*((np.dot(X, self.s_X.T) + self.theta)**self.d)\n",
    "        \n",
    "        # branch to correspond to category max, min\n",
    "        if self.y_max == 2:\n",
    "            if self.y_min == 1:\n",
    "                return np.where(np.dot(K, self.coef_*self.s_y)<0, 1, 2)\n",
    "            else:\n",
    "                return np.where(np.dot(K, self.coef_*self.s_y)<0, 0, 2)\n",
    "        else:\n",
    "            return np.where(np.dot(K, self.coef_*self.s_y)<0, 0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Problem 2] Determination of support vectors\n",
    "Treat samples whose computed Lagrangian multiplier λ is greater than a set threshold as support vectors. Support vectors are required when estimating. Write code to determine the support vectors and store them as instance variables.\n",
    "\n",
    "Threshold is a hyperparameter, but a good starting point is around 1e-5. If you can output the number of support vectors, you can check if the learning is going well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Question 3] Estimation\n",
    "At the time of estimation, the kernel function calculates the features of the data to be estimated and the features of the support vectors. The sign of f(x) obtained is the classification result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Question 4] Learning and Estimation\n",
    "Please learn and estimate the scratch implementation for binary classification of simple dataset 1 prepared in Sprint of Introduction to Machine Learning Scratch.\n",
    "\n",
    "Compare it with the scikit-learn implementation to see if it works.\n",
    "\n",
    "Please use scikit-learn for index values such as Accuracy, Precision, and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "y = pd.DataFrame(data=iris.target, columns=['species'])\n",
    "df = pd.concat([X, y], axis=1)\n",
    "# extraction\n",
    "df_2ex =df.query('species == 1 | species == 2')[[\"sepal length (cm)\", \"petal length (cm)\", 'species']]\n",
    "X = df_2ex[[\"sepal length (cm)\", \"petal length (cm)\",]]\n",
    "y = df_2ex[[\"species\"]]\n",
    "# ndarray conversion\n",
    "X_array = X.values\n",
    "# One-dimensionalization with respect to y (required for graphing)\n",
    "y_array = np.ravel(y.values)\n",
    "# Split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "     X_array, y_array, test_size=0.25, random_state=0)\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_valid_std = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6116\\1436362029.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# _init__(self, num_iter, lr, kernel='linear', gamma=1,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# theta=0, d=1, threshold=1e-5, verbose=False):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mssvc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScratchSVMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mssvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0my_valid_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mssvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6116\\445664696.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_iter, lr, kernel, gamma, theta, threshold, verbose)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Initialize parameters and predictors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mcoef_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "# learn\n",
    "\n",
    "# _init__(self, num_iter, lr, kernel='linear', gamma=1,\n",
    "# theta=0, d=1, threshold=1e-5, verbose=False):\n",
    "ssvc = ScratchSVMClassifier(5000, 0.01, threshold=1e-5)\n",
    "ssvc.fit(X_train_std, y_train, X_valid_std, y_valid)\n",
    "y_valid_predict = ssvc.predict(X_valid_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
